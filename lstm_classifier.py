from __future__ import absolute_import, division, print_function, unicode_literals
import numpy
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.utils import to_categorical

import warnings 

warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np
import datetime, os
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

#importing data
data = pd.read_csv(r'data/T2.csv', delimiter=",", error_bad_lines=False)
moriarty = pd.read_csv(r'data/Moriarty.csv', delimiter=",", error_bad_lines=False)
device_stats = pd.read_csv(r'data/T4.csv', delimiter=",", error_bad_lines=False)

#preparing data
device_stats = device_stats.sort_values(by=['UUID'])
X = device_stats
moriarty['Output'] = np.where(moriarty['SessionType'] == 'malicious', 1, 0)
Y = moriarty[['UUID', 'Output']]
Y.UUID = Y.UUID.astype('float')

Y = Y.drop(Y[Y.UUID < np.min(X.UUID)].index) #remove timestamps for events that happened before we have telemetry
Z = pd.merge_asof(left=X, right=Y, on='UUID', direction='nearest', tolerance=100000)
Z['Output'] = Z['Output'].fillna(0)

for i in list(Z):
    if Z[i].dtype == np.dtype('O'):
        Z = Z.drop([i], axis=1)

#Define Hyperparameters
NCOMPONENTS = 5
EPOCHS = np.linspace(4, 6, 3).astype('int')
WINDOW = [500]
BATCH=16

history_matrix = np.empty(np.size(EPOCHS), dtype=object)

for j in range(0, np.size(EPOCHS)):
    history_windows = np.empty(np.size(WINDOW), dtype=object)
    for k in range(0, np.size(WINDOW)):
        
        x_time_sz = X.UUID.shape[0]
        frame_sz = WINDOW[k]
        x_arr = X.UUID.to_numpy()
        iter_count = int(x_time_sz/frame_sz)
        result = np.zeros(iter_count)
        for i in range(0, iter_count):
            start = Y['UUID'] >= x_arr[frame_sz*i]
            end = Y['UUID'] < x_arr[frame_sz*(i+1)]
            result[i] = 1 if (np.sum(Y[start & end].Output)) > 0 else 0

        scaler = StandardScaler()
        dataset = Z.iloc[:iter_count*frame_sz, :]

        timesteps = WINDOW[k]
        dataset = dataset.to_numpy()

        dataset[np.isnan(dataset)] = 0
        scaler.fit(dataset)
        dataset_sc = scaler.transform(dataset)

        pca = PCA(n_components=10)
        pca.fit(dataset)

        # plt.plot(np.cumsum(pca.explained_variance_ratio_))
        # plt.xlabel('Number of components')
        # plt.ylabel('Cumulative explained variance')
        # plt.show()

        pca = PCA(n_components=NCOMPONENTS)
        dataset_pca = pca.fit_transform(dataset_sc)
        pca_std = np.std(dataset_pca)

        dataset_pca = np.reshape(dataset_pca, (iter_count, timesteps, dataset_pca.shape[1]))
        sz = dataset_pca.shape[0]

        train_size = 0.7
        split = int (sz*train_size)

        X_train = dataset_pca[:split, :, :]
        X_test = dataset_pca[split:, :, :]

        y_train = result[:split]
        y_test = result[split:]

        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)


        embedding_vecor_length = 32
        model = Sequential()

        model.add(LSTM(16, return_sequences=True, input_shape=(dataset_pca.shape[1], dataset_pca.shape[2])))
        model.add(LSTM(16, return_sequences=True))
        # model.add(LSTM(64, return_sequences=True))
        model.add(LSTM(16))
        model.add(Dense(2, activation='sigmoid'))

        model.compile(loss='categorical_crossentropy',
                      optimizer='rmsprop',
                      metrics=['accuracy'])

        print(model.summary())
        history = model.fit(X_train, y_train, epochs=EPOCHS[j], batch_size=BATCH, validation_data=(X_test, y_test))

        print('\n# Evaluate on test data')
        results = model.evaluate(X_test, y_test, batch_size=16)
        print('test loss, test acc:', results)
        
        history_windows[k] = history
    history_matrix[j] = history_windows


#evaluating the window size
epoch_label = ['EPOCH=4', 'EPOCH=5','EPOCH=6','EPOCH=7','EPOCH=8','EPOCH=9','EPOCH=10','EPOCH=11','EPOCH=12','EPOCH=13','EPOCH=14','EPOCH=15','EPOCH=16','EPOCH=17','EPOCH=18','EPOCH=19','EPOCH=20' ]
x_axis = WINDOW[:-1]
fig, ax = plt.subplots()
for i in range(0, np.size(history_matrix)):
    acc = np.empty(7)
    hist = history_matrix[i]
    for j in range(0, 7):
        acc[j] = hist[j].history['accuracy'][-1]
    ax.plot(x_axis, acc, label=epoch_label[i])

ax.legend()
plt.xlabel('Window Size')
plt.ylabel('Accuracy')

#evaluating training epochs
epoch5 = history_matrix[1]
win_size_label = ['WINDOW=100','WINDOW=200','WINDOW=250','WINDOW=300','WINDOW=500', 'WINDOW=1000', 'WINDOW=1500', 'WINDOW=2000']
epoch_x = np.linspace(1, 5, 5).astype('int')

fig, ax = plt.subplots()
for i in range(0, len(epoch5)-1):
    ax.plot(epoch_x, epoch5[i].history['accuracy'], label=win_size_label[i])
    
ax.legend()
plt.xticks(epoch_x)
plt.ylabel('Accuracy')
plt.xlabel("Training Epochs")

#evaluating validation accuracy
win_size_label = ['WINDOW=100','WINDOW=200','WINDOW=250','WINDOW=300','WINDOW=500', 'WINDOW=1000', 'WINDOW=1500']
epoch_x = np.linspace(1, 5, 5).astype('int')

fig, ax = plt.subplots()
for i in range(0, len(epoch5)-1):
    ax.plot(epoch_x, epoch5[i].history['val_accuracy'], label=win_size_label[i])
    
ax.legend()
plt.xticks(epoch_x)
plt.ylabel(' Validation Accuracy')
plt.xlabel("Training Epochs")